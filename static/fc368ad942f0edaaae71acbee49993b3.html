
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="TensorFlow, Machine Learning, Deep Learning, Artificial Intelligence">
    <meta name="keywords" content="TensorFlow, python,opencv">
    <title>用Tensorflow基于Deep Q Learning DQN 玩Flappy Bird</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <style>
        :root {
            --day-bg: #ffffff;
            --day-text: #333333;
            --night-bg: #1a1a1a;
            --night-text: #f0f0f0;
        }

        body {
            max-width: 820px;
            margin: 0 auto;
            padding: 20px;
            line-height: 21px;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Arial, sans-serif;
            transition: background-color 0.3s, color 0.3s;
        }

        .metadata {
            margin-bottom: 30px;
            padding: 15px;
            background: rgba(0, 0, 0, 0.05);
            border-radius: 5px;
        }

        .metadata p {
            margin: 5px 0;
            font-size: 0.9em;
        }

        .day-mode {
            background-color: var(--day-bg);
            color: var(--day-text);
        }

        .night-mode {
            background-color: var(--night-bg);
            color: var(--night-text);
        }

        h1 { font-size: 19px; text-align: center; }
        h2 { font-size: 18px; }
        h3 { font-size: 16px; }
        h4 { font-size: 14px; }
        h5 { font-size: 12px; }
        h6 { font-size: 11px; }

        pre {
            line-height: 24px;
            padding: 15px;
            border-radius: 5px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            overflow-x: auto;
        }

        code {
            font-family: 'Fira Code', Consolas, Monaco, 'Courier New', monospace;
        }

        img {
            display: block;
            max-width: 600px;
            height: auto;
            margin: 20px auto;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }

        .content {
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            padding: 20px;
            border-radius: 8px;
            background-color: rgba(255, 255, 255, 0.05);
        }

        footer {
            text-align: center;
            margin-top: 40px;
            padding: 20px;
            border-top: 1px solid #ddd;
            font-size: 0.9em;
        }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .content {
            animation: fadeIn 0.5s ease-in;
        }
    </style>
</head>
<body class="day-mode">
    <div class="metadata">
        <p><strong>Title:</strong> 用Tensorflow基于Deep Q Learning DQN 玩Flappy Bird</p>
        <p><strong>Categories:</strong> Machine Learning</p>
        <p><strong>Description:</strong> TensorFlow, Machine Learning, Deep Learning, Artificial Intelligence</p>
        <p><strong>Keywords:</strong> TensorFlow, python,opencv</p>
    </div>
    <div class="content">
        <p>发这个贴的原因，是因为 是因为 是因为啥来着，忘了 那就是因为你，因为你 。</p>
<p><a href="https://firsh.me/2017/07/04/tensorflow-node-2"><strong>上篇博文</strong></a>主要是TensorFlow的一个简单入门.</p>
<h2 id="tensorflow">前言 TensorFlow 通过深度学习自己去玩游戏，</h2>
<p>游戏图:
 <img alt="" src="http://112firshme11224.test.upcdn.net/tensorflow/tensorflow-game-bird.gif" /></p>
<p>整体思路：
 <img alt="" src="http://112firshme11224.test.upcdn.net/tensorflow/network.png" /></p>
<p>处理图：
 <img alt="" src="http://112firshme11224.test.upcdn.net/tensorflow/preprocess.png" /></p>
<h2 id="_1">后语</h2>
<p>能否使用DQN来实现通过屏幕学习玩Flappy Bird是一个有意思的挑战。（话说本人和朋友在前段时间也考虑了这个idea，但当时由于不知道如何截取游戏屏幕只能使用具体位置来学习，不过其实也成功了）</p>
<h2 id="_2">部分代码</h2>
<p>```python
 #!/usr/bin/env python
 from <strong>future</strong> import print_function</p>
<p>import tensorflow as tf
 import cv2
 import sys
 sys.path.append("game/")
 import wrapped_flappy_bird as game
 import random
 import numpy as np
 from collections import deque</p>
<p>GAME = 'bird' # the name of the game being played for log files
 ACTIONS = 2 # number of valid actions
 GAMMA = 0.99 # decay rate of past observations
 OBSERVE = 100000. # timesteps to observe before training
 EXPLORE = 2000000. # frames over which to anneal epsilon
 FINAL_EPSILON = 0.0001 # final value of epsilon
 INITIAL_EPSILON = 0.0001 # starting value of epsilon
 REPLAY_MEMORY = 50000 # number of previous transitions to remember
 BATCH = 32 # size of minibatch
 FRAME_PER_ACTION = 1</p>
<p>def weight_variable(shape):
     initial = tf.truncated_normal(shape, stddev = 0.01)
     return tf.Variable(initial)</p>
<p>def bias_variable(shape):
     initial = tf.constant(0.01, shape = shape)
     return tf.Variable(initial)</p>
<p>def conv2d(x, W, stride):
     return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = "SAME")</p>
<p>def max_pool_2x2(x):
     return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = "SAME")</p>
<p>def createNetwork():
     # network weights
     W_conv1 = weight_variable([8, 8, 4, 32])
     b_conv1 = bias_variable([32])</p>
<pre class="codehilite"><code> W_conv2 = weight_variable([4, 4, 32, 64])
 b_conv2 = bias_variable([64])

 W_conv3 = weight_variable([3, 3, 64, 64])
 b_conv3 = bias_variable([64])

 W_fc1 = weight_variable([1600, 512])
 b_fc1 = bias_variable([512])

 W_fc2 = weight_variable([512, ACTIONS])
 b_fc2 = bias_variable([ACTIONS])

 # input layer
 s = tf.placeholder(&quot;float&quot;, [None, 80, 80, 4])

 # hidden layers
 h_conv1 = tf.nn.relu(conv2d(s, W_conv1, 4) + b_conv1)
 h_pool1 = max_pool_2x2(h_conv1)

 h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 2) + b_conv2)
 #h_pool2 = max_pool_2x2(h_conv2)

 h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1) + b_conv3)
 #h_pool3 = max_pool_2x2(h_conv3)

 #h_pool3_flat = tf.reshape(h_pool3, [-1, 256])
 h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])

 h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)

 # readout layer
 readout = tf.matmul(h_fc1, W_fc2) + b_fc2

 return s, readout, h_fc1
</code></pre>

<p>def trainNetwork(s, readout, h_fc1, sess):
     # define the cost function
     a = tf.placeholder("float", [None, ACTIONS])
     y = tf.placeholder("float", [None])
     readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)
     cost = tf.reduce_mean(tf.square(y - readout_action))
     train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)</p>
<pre class="codehilite"><code> # open up a game state to communicate with emulator
 game_state = game.GameState()

 # store the previous observations in replay memory
 D = deque()

 # printing
 a_file = open(&quot;logs_&quot; + GAME + &quot;/readout.txt&quot;, 'w')
 h_file = open(&quot;logs_&quot; + GAME + &quot;/hidden.txt&quot;, 'w')

 # get the first state by doing nothing and preprocess the image to 80x80x4
 do_nothing = np.zeros(ACTIONS)
 do_nothing[0] = 1
 x_t, r_0, terminal = game_state.frame_step(do_nothing)
 x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)
 ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)
 s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)

 # saving and loading networks
 saver = tf.train.Saver()
 sess.run(tf.initialize_all_variables())
 checkpoint = tf.train.get_checkpoint_state(&quot;saved_networks&quot;)
 if checkpoint and checkpoint.model_checkpoint_path:
     saver.restore(sess, checkpoint.model_checkpoint_path)
     print(&quot;Successfully loaded:&quot;, checkpoint.model_checkpoint_path)
 else:
     print(&quot;Could not find old network weights&quot;)

 # start training
 epsilon = INITIAL_EPSILON
 t = 0
 while &quot;flappy bird&quot; != &quot;angry bird&quot;:
     # choose an action epsilon greedily
     readout_t = readout.eval(feed_dict={s : [s_t]})[0]
     a_t = np.zeros([ACTIONS])
     action_index = 0
     if t % FRAME_PER_ACTION == 0:
         if random.random() &lt;= epsilon:
             print(&quot;----------Random Action----------&quot;)
             action_index = random.randrange(ACTIONS)
             a_t[random.randrange(ACTIONS)] = 1
         else:
             action_index = np.argmax(readout_t)
             a_t[action_index] = 1
     else:
         a_t[0] = 1 # do nothing

     # scale down epsilon
     if epsilon &gt; FINAL_EPSILON and t &gt; OBSERVE:
         epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE

     # run the selected action and observe next state and reward
     x_t1_colored, r_t, terminal = game_state.frame_step(a_t)
     x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)
     ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)
     x_t1 = np.reshape(x_t1, (80, 80, 1))
     #s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)
     s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)

     # store the transition in D
     D.append((s_t, a_t, r_t, s_t1, terminal))
     if len(D) &gt; REPLAY_MEMORY:
         D.popleft()

     # only train if done observing
     if t &gt; OBSERVE:
         # sample a minibatch to train on
         minibatch = random.sample(D, BATCH)

         # get the batch variables
         s_j_batch = [d[0] for d in minibatch]
         a_batch = [d[1] for d in minibatch]
         r_batch = [d[2] for d in minibatch]
         s_j1_batch = [d[3] for d in minibatch]

         y_batch = []
         readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})
         for i in range(0, len(minibatch)):
             terminal = minibatch[i][4]
             # if terminal, only equals reward
             if terminal:
                 y_batch.append(r_batch[i])
             else:
                 y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))

         # perform gradient step
         train_step.run(feed_dict = {
             y : y_batch,
             a : a_batch,
             s : s_j_batch}
         )

     # update the old values
     s_t = s_t1
     t += 1

     # save progress every 10000 iterations
     if t % 10000 == 0:
         saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)

     # print info
     state = &quot;&quot;
     if t &lt;= OBSERVE:
         state = &quot;observe&quot;
     elif t &gt; OBSERVE and t &lt;= OBSERVE + EXPLORE:
         state = &quot;explore&quot;
     else:
         state = &quot;train&quot;

     print(&quot;TIMESTEP&quot;, t, &quot;/ STATE&quot;, state, \
         &quot;/ EPSILON&quot;, epsilon, &quot;/ ACTION&quot;, action_index, &quot;/ REWARD&quot;, r_t, \
         &quot;/ Q_MAX %e&quot; % np.max(readout_t))
     # write info to files
     '''
     if t % 10000 &lt;= 100:
         a_file.write(&quot;,&quot;.join([str(x) for x in readout_t]) + '\n')
         h_file.write(&quot;,&quot;.join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\n')
         cv2.imwrite(&quot;logs_tetris/frame&quot; + str(t) + &quot;.png&quot;, x_t1)
     '''
</code></pre>

<p>def playGame():
     sess = tf.InteractiveSession()
     s, readout, h_fc1 = createNetwork()
     trainNetwork(s, readout, h_fc1, sess)</p>
<p>def main():
     playGame()</p>
<p>if <strong>name</strong> == "<strong>main</strong>":
     main()
 ```</p>
<blockquote>
<p>一切的学习都是为了以前吹过的牛皮啊～</p>
</blockquote>
    </div>
    <footer>
        Power By Python Markdown Generate 2024-11-11 11:02:36
    </footer>
    <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            document.querySelectorAll('pre code').forEach((block) => {
                hljs.highlightBlock(block);
            });
        });

        function setThemeMode() {
            const hour = new Date().getHours();
            const body = document.body;
            if (hour >= 18 || hour < 6) {
                body.classList.remove('day-mode');
                body.classList.add('night-mode');
            } else {
                body.classList.remove('night-mode');
                body.classList.add('day-mode');
            }
        }

        setThemeMode();
        setInterval(setThemeMode, 60000);
    </script>
</body>
</html>
