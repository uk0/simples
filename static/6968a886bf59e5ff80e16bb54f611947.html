<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>2017-08-02-hdfs-flink-sink.md</title>

<link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
<style>
    html{--static-white:#fff;--bg-body:#fff;--bg-filler:#eff0f1;--bg-body-overlay:#f5f6f7;--text-title:#1f2329;--text-link-hover:#3370ff;--text-caption:#646a73;--text-placeholder:#8f959e;--primary-pri-400:#4e83fd;--primary-pri-500:#3370ff;--primary-pri-600:#245bdb;--line-border-card:#dee0e3;--shadow-default-sm:rgba(31, 35, 41, 0.12);--ccmtoken-mindnote-highlightcolor-neutral-raw:222,224,227;--icon-n1:#2b2f36;--icon-n2:#646a73}html[data-theme=dark]{--static-white:#fff;--bg-body:#1a1a1a;--bg-filler:#373737;--bg-body-overlay:#2e2e2e;--text-title:#f0f0f0;--text-link-hover:#4c88ff;--text-caption:#a6a6a6;--text-placeholder:#939393;--primary-pri-400:#2e65d1;--primary-pri-500:#4c88ff;--primary-pri-600:#70a0ff;--line-border-card:#5f5f5f;--shadow-default-sm:rgba(0, 0, 0, 0.32);--ccmtoken-mindnote-highlightcolor-neutral-raw:80,80,80;--icon-n1:#e8e8e8;--icon-n2:#a6a6a6}
    body
    {
        width:auto;
         
        font-family: LarkHackSafariFont,LarkEmojiFont,LarkChineseQuote,-apple-system,BlinkMacSystemFont,Helvetica Neue,Tahoma,PingFang SC,Microsoft Yahei,Arial,Hiragino Sans GB,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;
        font-size:14px;
        color: #312222;
        line-height:23px;
        letter-spacing:1px
    }
    .word{
        margin-top:5%; margin-left: 5%; margin-right: 9%;
        word-wrap:break-word;
    }
</style>
</head>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
<script>
    hljs.highlightAll();
document.querySelectorAll('div.code').forEach(el => {
    
    hljs.highlightElement(el);

});
</script>

<body style="background-color: #FFFFFF">
<div class = "word">
    <hr />  
  
<p>layout: post<br />  
title:  hadoop HDFS作为Flink 的 Sink<br />  
categories: shell,hadoop,hdfs<br />  
description: 笔记</p>  
  
<h2>keywords: hdfs,flink</h2>  
  
<p>因工作需求，学习了Hdfs分布式文件存储系统，所整合Flink + HDFS 作为一个Demo 帮助大家跳坑。<br />  
HDFS 采用NS高可用模式。</p>  
  
<h1>HDFSManager.Java</h1>  
  
<pre><code>* 初始化HDFS链接。  
</code></pre>  
  
<pre><code class="language-java">package com.e.firsh.spb.utils.hdfs;  
  
import com.alibaba.fastjson.JSONObject;  
import com.e.firsh.base.esb.BO;  
import com.e.firsh.base.utils.Registry;  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.FSDataOutputStream;  
import org.apache.hadoop.fs.FileSystem;  
import org.apache.hadoop.fs.Path;  
import org.slf4j.Logger;  
import org.slf4j.LoggerFactory;  
import java.io.IOException;  
import java.util.Set;  
  
/**  
 * The type Hdfs manager.  
 */  
public class HDFSManager {  
    private static Logger logger = LoggerFactory.getLogger(HDFSManager.class);  
    private static Configuration configuration;  
    private static FileSystem fs;  
  
    public HDFSManager() {  
        JSONObject jsonObject = new JSONObject();  
        jsonObject.put(&quot;fs.defaultFS&quot;, &quot;hdfs://ns&quot;);  
        jsonObject.put(&quot;dfs.nameservices&quot;, &quot;ns&quot;);  
        jsonObject.put(&quot;dfs.ha.namenodes.ns&quot;,&quot;nn1,nn2&quot;);  
        jsonObject.put(&quot;dfs.namenode.rpc-address.ns.nn1&quot;, &quot;10.11.0.12:9000&quot;);  
        jsonObject.put(&quot;dfs.namenode.rpc-address.ns.nn2&quot;, &quot;10.11.0.13:9000&quot;);  
        jsonObject.put(&quot;dfs.client.failover.proxy.provider.ns&quot;, &quot;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&quot;);  
        init(jsonObject);  
    }  
  
    public boolean init(JSONObject args) {  
        Configuration conf = new Configuration();  
        Set&lt;String&gt; itr = args.keySet();  
        if (itr != null) {  
            for (String pname : itr) {  
                String pvalue = args.getString(pname);  
                conf.set(pname, pvalue);  
            }  
        }  
        try {  
            fs = FileSystem.get(conf);  
            return true;  
        } catch (Exception e) {  
            logger.error(e.getMessage() + &quot;:{}&quot;, e);  
        }  
        return false;  
    }  
  
  
    public BO appendToFile(String destPath, String line) throws IOException {  
        BO respBO = BO.createResponseBO();  
        Path path = new Path(destPath);  
        FSDataOutputStream dos = null;  
        try {  
            if (!fs.exists(path)) {  
                dos = fs.create(path);  
            } else {  
                dos = fs.append(path);  
            }  
            byte[] readBuf = line.getBytes(&quot;UTF-8&quot;);  
            dos.write(readBuf, 0, readBuf.length);  
            dos.close();  
            respBO.setDataNameValue(&quot;length&quot;, readBuf.length);  
        } catch (Exception e) {  
            logger.error(e.getMessage() + &quot;:{}&quot;, e);  
            respBO.setCompleteCode(BO.BO_V_CC_FAILED);  
        } finally {  
            if (dos != null) {  
                dos.close();  
            }  
        }  
        return respBO;  
    }  
  
    public static void main(String[] args) {  
        HDFSManager hdfsManager = new HDFSManager();  
        try {  
            hdfsManager.appendToFile(&quot;/testmq&quot;,&quot;hello w&quot;);  
        } catch (IOException e) {  
            e.printStackTrace();  
        }  
    }  
  
    public static void init2(){  
         configuration = new Configuration();  
        /**TODO 添加Hadoop配置内容*/  
        configuration.set(&quot;dfs.namenode.name.dir&quot;, &quot;file:///home/admin/data/hadoop/hdfs/name&quot;);  
        configuration.set(&quot;dfs.nameservices&quot;, &quot;ns&quot;);  
        configuration.set(&quot;dfs.ha.namenodes.ns&quot;, &quot;nn1,nn2&quot;);  
        configuration.set(&quot;dfs.namenode.rpc-address.ns.nn1&quot;, &quot;10.11.0.12:9000&quot;);  
        configuration.set(&quot;dfs.namenode.rpc-address.ns.nn2&quot;, &quot;10.11.0.13:9000&quot;);  
        configuration.set(&quot;dfs.namenode.shared.edits.dir&quot;, &quot;qjournal://10.11.0.12:8485;10.11.0.13:8485;10.11.0.14:8485/ns&quot;);  
        configuration.set(&quot;hadoop.tmp.dir&quot;, &quot;/home/admin/data/hadoop/tmp&quot;);  
        configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://ns&quot;);  
        configuration.set(&quot;dfs.journalnode.edits.dir&quot;, &quot;/home/admin/data/hadoop/journal&quot;);  
        configuration.set(&quot;dfs.client.failover.proxy.provider.ns&quot;, &quot;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&quot;);  
        configuration.set(&quot;ha.zookeeper.quorum&quot;, &quot;10.11.0.12:2181,10.11.0.13:2181,10.11.0.14:2181&quot;);  
        configuration.set(&quot;mapreduce.input.fileinputformat.split.minsize&quot;, &quot;10&quot;);  
  
  
        try {  
            fs = FileSystem.get(configuration);  
        } catch (Exception e) {  
            logger.error(e.getMessage() + &quot;:{}&quot;, e);  
        }  
    }  
}  
  
</code></pre>  
  
<h1>HDFSSink.Java</h1>  
  
<ul>  
<li>继承<code>RichSinkFunction</code>，重写<code>open</code> 和 <code>invoke</code>方法，还有<code>close</code>。<br />  
</li>  
</ul>  
  
<pre><code class="language-java">package com.e.firsh.spb.sink;  
  
import com.e.firsh.spb.utils.hdfs.HDFSManager;  
import org.apache.flink.configuration.Configuration;  
import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;  
  
/**  
 * Created by zhangjianxin on 2017/8/1.  
 */  
public class HDFSSink extends RichSinkFunction&lt;String&gt; {  
    private HDFSManager hdfsManager;  
    private final static  String  HDFS_PATH =&quot;/testmq&quot;;  
    @Override  
    public void invoke(String t) throws Exception {  
        hdfsManager.appendToFile(HDFS_PATH,t);  
    }  
    @Override  
    public void open(Configuration config) {  
        hdfsManager = new HDFSManager();  
    }  
}  
  
</code></pre>  
  
<h1>以上代码为简易版经过修改，思路清晰，（Flink自带一种HDFS的链接方式详情见链接：）</h1>  
  
<ul>  
<li><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/filesystem_sink.html">https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/connectors/filesystem_sink.html</a></li>  
</ul>  

</div>

</body>

</html>