<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>2017-08-02-hdfs-shell-bash.md</title>

<link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
<style>
    html{--static-white:#fff;--bg-body:#fff;--bg-filler:#eff0f1;--bg-body-overlay:#f5f6f7;--text-title:#1f2329;--text-link-hover:#3370ff;--text-caption:#646a73;--text-placeholder:#8f959e;--primary-pri-400:#4e83fd;--primary-pri-500:#3370ff;--primary-pri-600:#245bdb;--line-border-card:#dee0e3;--shadow-default-sm:rgba(31, 35, 41, 0.12);--ccmtoken-mindnote-highlightcolor-neutral-raw:222,224,227;--icon-n1:#2b2f36;--icon-n2:#646a73}html[data-theme=dark]{--static-white:#fff;--bg-body:#1a1a1a;--bg-filler:#373737;--bg-body-overlay:#2e2e2e;--text-title:#f0f0f0;--text-link-hover:#4c88ff;--text-caption:#a6a6a6;--text-placeholder:#939393;--primary-pri-400:#2e65d1;--primary-pri-500:#4c88ff;--primary-pri-600:#70a0ff;--line-border-card:#5f5f5f;--shadow-default-sm:rgba(0, 0, 0, 0.32);--ccmtoken-mindnote-highlightcolor-neutral-raw:80,80,80;--icon-n1:#e8e8e8;--icon-n2:#a6a6a6}
    body
    {
        width:auto; margin-top:5%; margin-left: 21%; margin-right: 26%;
        float:right;
         
        font-family: LarkHackSafariFont,LarkEmojiFont,LarkChineseQuote,-apple-system,BlinkMacSystemFont,Helvetica Neue,Tahoma,PingFang SC,Microsoft Yahei,Arial,Hiragino Sans GB,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;
        font-size:14px;
        color: #312222;
        line-height:23px;
        letter-spacing:1px
    }
</style>
</head>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
<script>
    hljs.highlightAll();
document.querySelectorAll('div.code').forEach(el => {
    
    hljs.highlightElement(el);

});
</script>

<body style="background-color: #FFFFFF">
<hr />  
  
<p>layout: post<br />  
title:  hadoop HDFS常用文件操作命令<br />  
categories: shell,hadoop,hdfs<br />  
description: 笔记</p>  
  
<h2>keywords: shell,hdfs</h2>  
  
<p>工作当中接触到了 hadoop HDFS,操作命令有些不熟悉，所以记录下来以备不时之需。</p>  
  
<h2>命令基本格式:</h2>  
  
<pre><code class="language-bash">hadoop fs -cmd &lt; args &gt;  
</code></pre>  
  
<h1>1.ls</h1>  
  
<ul>  
<li>列出hdfs文件系统根目录下的目录和文件<br />  
</li>  
</ul>  
  
<pre><code class="language-bash">hadoop fs -ls  /  
  
</code></pre>  
  
<ul>  
<li>列出hdfs文件系统所有的目录和文件<br />  
</li>  
</ul>  
  
<pre><code class="language-bash">hadoop fs -ls -R /  
</code></pre>  
  
<h1>2.put</h1>  
  
<ul>  
<li><p>hdfs file的父目录一定要存在，否则命令不会执行</p>  
  
<pre><code class="language-bash">hadoop fs -put &lt; local file &gt; &lt; hdfs file &gt;  
</code></pre>  
<ul>  
<li>hdfs dir 一定要存在，否则命令不会执行<br />  
<code>bash  
hadoop fs -put  &lt; local file or dir &gt;...&lt; hdfs dir &gt;  
</code><br />  
</li>  
</ul></li>  
  
<li><p>从键盘读取输入到hdfs file中，按Ctrl+D结束输入，hdfs file不能存在，否则命令不会执行</p></li>  
</ul>  
  
<pre><code class="language-bash">hadoop fs -put - &lt; hdsf  file&gt;  
</code></pre>  
  
<h1>2.1.moveFromLocal</h1>  
  
<ul>  
<li>与put相类似，命令执行后源文件 local src 被删除，也可以从从键盘读取输入到hdfs file中<br />  
</li>  
</ul>  
  
<pre><code class="language-bash">hadoop fs -moveFromLocal  &lt; local src &gt; ... &lt; hdfs dst &gt;  
</code></pre>  
  
<h1>2.2.copyFromLocal</h1>  
  
<ul>  
<li>与put相类似，也可以从从键盘读取输入到hdfs file中<br />  
</li>  
</ul>  
  
<pre><code class="language-bash">hadoop fs -copyFromLocal  &lt; local src &gt; ... &lt; hdfs dst &gt;  
</code></pre>  
  
<h1>3.get</h1>  
  
<ul>  
<li>local file不能和 hdfs file名字不能相同，否则会提示文件已存在，没有重名的文件会复制到本地<br />  
</li>  
</ul>  
  
<pre><code class="language-bash">hadoop fs -get &lt; hdfs file &gt; &lt; local file or dir&gt;  
</code></pre>  
  
<ul>  
<li>拷贝多个文件或目录到本地时，本地要为文件夹路径<br />  
<code>注意</code>：如果用户不是root， local 路径要为用户文件夹下的路径，否则会出现权限问题<br />  
</li>  
</ul>  
  
<pre><code class="language-bash">hadoop fs -get &lt; hdfs file or dir &gt; ... &lt; local  dir &gt;  
</code></pre>  
  
<h1>3.1.copyToLocal</h1>  
  
<pre><code class="language-bash">hadoop fs -copyToLocal &lt; local src &gt; ... &lt; hdfs dst &gt;  
</code></pre>  
  
<h1>4.rm</h1>  
  
<ul>  
<li>每次可以删除多个文件或目录<br />  
</li>  
</ul>  
  
<pre><code class="language-bash">hadoop fs -rm &lt; hdfs file &gt; ...  
hadoop fs -rm -r &lt; hdfs dir&gt;...  
</code></pre>  
  
<h1>5.mkdir</h1>  
  
<pre><code class="language-bash"># 只能一级一级的建目录，父目录不存在的话使用这个命令会报错  
hadoop fs -mkdir &lt; hdfs path&gt;  
# 所创建的目录如果父目录不存在就创建该父目录  
hadoop fs -mkdir -p &lt; hdfs path&gt;  
</code></pre>  
  
<h1>6.getmerge</h1>  
  
<pre><code class="language-bash"># 将hdfs指定目录下所有文件排序后合并到local指定的文件中，文件不存在时会自动创建，文件存在时会覆盖里面的内容  
hadoop fs -getmerge &lt; hdfs dir &gt;  &lt; local file &gt;  
  
# 加上nl后，合并到local file中的hdfs文件之间会空出一行  
  
hadoop fs -getmerge -nl  &lt; hdfs dir &gt;  &lt; local file &gt;  
</code></pre>  
  
<h1>7.cp</h1>  
  
<pre><code class="language-bash"> hadoop fs -cp  &lt; hdfs file &gt;  &lt; hdfs file &gt;  
# 目标文件不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件还存在  
 hadoop fs -cp &lt; hdfs file or dir &gt;... &lt; hdfs dir &gt;  
# 目标文件夹要存在，否则命令不能执行  
</code></pre>  
  
<h1>8.mv</h1>  
  
<pre><code class="language-bash">hadoop fs -mv &lt; hdfs file &gt;  &lt; hdfs file &gt;  
# 目标文件不能存在，否则命令不能执行，相当于给文件重命名并保存，源文件不存在  
hadoop fs -mv  &lt; hdfs file or dir &gt;...  &lt; hdfs dir &gt;  
  
# 源路径有多个时，目标路径必须为目录，且必须存在。  
#  `注意`：跨文件系统的移动（local到hdfs或者反过来）都是不允许的  
</code></pre>  
  
<h1>9.count</h1>  
  
<pre><code class="language-bash"># 统计hdfs对应路径下的目录个数，文件个数，文件总计大小  
# 显示为目录个数，文件个数，文件总计大小，输入路径  
hadoop fs -count &lt; hdfs path &gt;  
</code></pre>  
  
<h1>10.du</h1>  
  
<ul>  
<li>显示hdfs对应路径下每个文件夹和文件的大小<br />  
<code>bash  
hadoop fs -du &lt; hdsf path&gt;  
</code><br />  
* 显示hdfs对应路径下所有文件和的大小<br />  
<code>bash  
hadoop fs -du -s &lt; hdsf path&gt;  
</code><br />  
</li>  
<li>显示hdfs对应路径下每个文件夹和文件的大小,文件的大小用方便阅读的形式表示，例如用64M代替67108864<br />  
<code>bash  
hadoop fs -du - h &lt; hdsf path&gt;  
</code><br />  
# 11.text<br />  
* 将文本文件或某些格式的非文本文件通过文本格式输出<br />  
<code>bash  
hadoop fs -text &lt; hdsf file&gt;  
</code><br />  
</li>  
</ul>  
  
<h1>12.setrep</h1>  
  
<ul>  
<li>改变一个文件在hdfs中的副本个数，上述命令中数字3为所设置的副本个数，-R选项可以对一个人目录下的所有目录+文件递归执行改变副本个数的操作<br />  
</li>  
</ul>  
  
<pre><code class="language-bash">hadoop fs -setrep -R 3 &lt; hdfs path &gt;  
</code></pre>  
  
<h1>13.stat</h1>  
  
<ul>  
<li>返回对应路径的状态信息<br />  
<code>[format]</code>可选参数有：<code>%b</code>（文件大小），<code>%o</code>（Block大小），<code>%n</code>（文件名），<code>%r</code>（副本个数），<code>%y</code>（最后一次修改日期和时间）<br />  
可以这样书写<code>hadoop fs -stat</code> %b%o%n &lt; hdfs path &gt;，不过不建议，这样每个字符输出的结果不是太容易分清楚<br />  
</li>  
</ul>  
  
<pre><code class="language-bash">hdoop fs -stat [format] &lt; hdfs path &gt;  
</code></pre>  
  
<h1>14.tail</h1>  
  
<ul>  
<li><p>在标准输出中显示文件末尾的<code>1KB</code>数据</p>  
  
<pre><code class="language-bash">hadoop fs -tail &lt; hdfs file &gt;  
</code></pre>  
<h1>15.archive</h1>  
  
<pre><code class="language-bash">hadoop archive -archiveName name.har -p &lt; hdfs parent dir &gt; &lt; src &gt;* &lt; hdfs dst &gt;  
</code></pre></li>  
  
<li><p>命令中参数name：压缩文件名，自己任意取；&lt; hdfs parent dir &gt; ：压缩文件所在的父目录；<code>&lt; src &gt;</code>：要压缩的文件名；<code>&lt; hdfs dst &gt;</code>：压缩文件存放路径<br />  
*示例：<code>hadoop archive</code> -archiveName hadoop.har -p /user 1.txt 2.txt /des<br />  
示例中将hdfs中/user目录下的文件<code>1.txt</code>，<code>2.txt</code>压缩成一个名叫<code>hadoop.har</code>的文件存放在<code>hdfs</code>中/des目录下，如果1.txt，2.txt不写就是将/user目录下所有的目录和文件压缩成一个名叫hadoop.har的文件存放在hdfs中/des目录下<br />  
显示har的内容可以用如下命令：</p></li>  
</ul>  
  
<pre><code class="language-bash">hadoop fs -ls /des/flink-des.jar  
</code></pre>  
  
<ul>  
<li>显示har压缩的是那些文件可以用如下命令<br />  
</li>  
</ul>  
  
<pre><code class="language-bash">hadoop fs -ls -R har:///des/hadoop.har  
</code></pre>  
  
<ul>  
<li>注意：<code>har</code>文件不能进行二次压缩。如果想给.har加文件，只能找到原来的文件，重新创建一个。har文件中原来文件的数据并没有变化，har文件真正的作用是减少<code>NameNode</code>和<code>DataNode</code>过多的空间浪费。</li>  
</ul>  


</body>

</html>